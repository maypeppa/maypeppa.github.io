<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-31377772-3"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-31377772-3');</script>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Apache Hadoop Goes Realtime at Facebook</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="work" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/styles/readtheorg/css/htmlize.css"/><link rel="stylesheet" type="text/css" href="/themes/styles/readtheorg/css/readtheorg.css"/><script src="https://ajax.loli.net/ajax/libs/jquery/2.1.3/jquery.min.js"></script><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script><script type="text/javascript" src="/themes/styles/lib/js/jquery.stickytableheaders.min.js"></script><script type="text/javascript" src="/themes/styles/readtheorg/js/readtheorg.js"></script></head>
<body>
<div id="content">
<h1 class="title">Apache Hadoop Goes Realtime at Facebook</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org86d5320">1. ABSTRACT</a></li>
<li><a href="#orgf565d4b">2. INTRODUCTION</a></li>
<li><a href="#org4a5f547">3. WORKLOAD TYPES</a>
<ul>
<li><a href="#org57f84ed">3.1. Facebook Messaging</a>
<ul>
<li><a href="#org2315fca">3.1.1. High Write Throughput</a></li>
<li><a href="#org7cf2646">3.1.2. Large Tables</a></li>
<li><a href="#org0b9a252">3.1.3. Data Migration</a></li>
</ul>
</li>
<li><a href="#org6613a75">3.2. Facebook Insights</a>
<ul>
<li><a href="#org88597e9">3.2.1. Realtime Analytics</a></li>
<li><a href="#orgc290388">3.2.2. High Throughput Increments</a></li>
</ul>
</li>
<li><a href="#orgf594f66">3.3. Facebook Metrics System (ODS)</a>
<ul>
<li><a href="#orgbd83a5f">3.3.1. Automatic Sharding</a></li>
<li><a href="#orga999883">3.3.2. Fast Reads of Recent Data and Table Scans</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org07bc912">4. WHY HADOOP AND HBASE</a></li>
<li><a href="#org64e000d">5. REALTIME HDFS</a>
<ul>
<li><a href="#org91fe6c5">5.1. High Availability - AvatarNode</a></li>
<li><a href="#orged531f7">5.2. Hadoop RPC compatibility</a></li>
<li><a href="#org000d9e4">5.3. Block Availability: Placement Policy</a></li>
<li><a href="#org1f53e6f">5.4. Performance Improvements for a Realtime Workload</a>
<ul>
<li><a href="#org24efa31">5.4.1. RPC Timeout</a></li>
<li><a href="#orgd9b4178">5.4.2. Recover File Lease</a></li>
<li><a href="#orgc6e30dc">5.4.3. Reads from Local Replicas</a></li>
</ul>
</li>
<li><a href="#org1d83926">5.5. New Features</a>
<ul>
<li><a href="#org0b3147e">5.5.1. HDFS sync</a></li>
<li><a href="#org805cf07">5.5.2. Concurrent Readers</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org01e7a69">6. PRODUCTION HBASE</a>
<ul>
<li><a href="#orgb339aca">6.1. ACID Compliance</a></li>
<li><a href="#orgcf51594">6.2. Availability Improvements</a>
<ul>
<li><a href="#org357088b">6.2.1. HBase Master Rewrite</a></li>
<li><a href="#orge97d018">6.2.2. Online Upgrades</a></li>
<li><a href="#org08ef837">6.2.3. Distributed Log Splitting</a></li>
</ul>
</li>
<li><a href="#org9e53f37">6.3. Performance Improvements</a>
<ul>
<li><a href="#org6659658">6.3.1. Compaction</a></li>
<li><a href="#orgedc5a9e">6.3.2. Read Optimizations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org35c7520">7. DEPLOYMENT AND OPERATIONAL EXPERIENCES</a>
<ul>
<li><a href="#orge7570eb">7.1. Testing</a></li>
<li><a href="#orgf7b5f91">7.2. Monitoring and Tools</a></li>
<li><a href="#orgd7df8e3">7.3. Manual versus Automatic Splitting</a></li>
<li><a href="#orgcfbd9a5">7.4. Dark Launch</a></li>
<li><a href="#orgaa44bd6">7.5. Dashboards/ODS integration</a></li>
<li><a href="#orgae4dc9d">7.6. Backups at the Application layer</a></li>
<li><a href="#org94351b1">7.7. Schema Changes</a></li>
<li><a href="#org77ab2df">7.8. Importing Data</a></li>
<li><a href="#org224dad2">7.9. Reducing Network IO</a></li>
</ul>
</li>
<li><a href="#org42c0b49">8. FUTURE WORK</a></li>
</ul>
</div>
</div>
<p>
<a href="http://borthakur.com/ftp/RealtimeHadoopSigmod2011.pdf">http://borthakur.com/ftp/RealtimeHadoopSigmod2011.pdf</a>
</p>

<div id="outline-container-org86d5320" class="outline-2">
<h2 id="org86d5320"><span class="section-number-2">1</span> ABSTRACT</h2>
</div>
<div id="outline-container-orgf565d4b" class="outline-2">
<h2 id="orgf565d4b"><span class="section-number-2">2</span> INTRODUCTION</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>At Facebook, Hadoop has traditionally been used in conjunction with Hive for storage and analysis of large data sets. Most of this analysis occurs in offline batch jobs and the emphasis has been on maximizing throughput and efficiency. These workloads typically read and write large amounts of data from disk sequentially. As such, there has been less emphasis on making Hadoop performant for random access workloads by providing low latency access to HDFS. （早期Hadoop只是用于离线分析，顺序读写磁盘，没有关注通过降低访问HDFS的延迟来提升随机访问的性能）Instead, we have used a combination of large clusters of MySQL databases and caching tiers built using memcached. In many cases, results from Hadoop are uploaded into MySQL or memcached for consumption by the web tier.（对于在线数据访问是将Hadoop数据upload到MySQL, 然后配合memcached来完成的）</li>
<li>Recently, a new generation of applications has arisen at Facebook that require very high write throughput and cheap and elastic storage, while simultaneously requiring low latency and disk efficient sequential and random read performance. MySQL storage engines are proven and have very good random read performance, but typically suffer from low random write throughput. It is difficult to scale up our MySQL clusters rapidly while maintaining good load balancing and high uptime. Administration of MySQL clusters requires a relatively high management overhead and they typically use more expensive hardware. Given our high confidence in the reliability and scalability of HDFS, we began to explore Hadoop and HBase for such applications.（MySQL的随机读性能不错，但是随机写性能非常差，同时在扩展性上也不太好。管理集群需要非常高的额外代价，并且在硬件使用方面都是高端机器）
<ul class="org-ul">
<li>The first set of applications requires realtime concurrent, but sequential, read access to a very large stream of realtime data being stored in HDFS. （HDFS必须支持高并发的实时读写需求）An example system generating and storing such data is Scribe , an open source distributed log aggregation service created by and used extensively at Facebook. Previously, data generated by Scribe was stored in expensive and hard to manage NFS servers.</li>
<li>The second generation of non-MapReduce Hadoop applications needed to dynamically index a rapidly growing data set for fast random lookups. In addition, this new application had to be suited for production use by more than 500 million people immediately after launch and needed to scale to many petabytes of data with stringent uptime requirements. We decided to use HBase for this project. HBase in turn leverages HDFS for scalable and fault tolerant storage and ZooKeeper for distributed consensus.（HBase用来存储索引并且提供实时的随机查询）</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org4a5f547" class="outline-2">
<h2 id="org4a5f547"><span class="section-number-2">3</span> WORKLOAD TYPES</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org57f84ed" class="outline-3">
<h3 id="org57f84ed"><span class="section-number-3">3.1</span> Facebook Messaging</h3>
<div class="outline-text-3" id="text-3-1">
<p>
存储用户各种消息如email,chat,SMS等。
</p>
</div>

<div id="outline-container-org2315fca" class="outline-4">
<h4 id="org2315fca"><span class="section-number-4">3.1.1</span> High Write Throughput</h4>
</div>
<div id="outline-container-org7cf2646" class="outline-4">
<h4 id="org7cf2646"><span class="section-number-4">3.1.2</span> Large Tables</h4>
</div>
<div id="outline-container-org0b9a252" class="outline-4">
<h4 id="org0b9a252"><span class="section-number-4">3.1.3</span> Data Migration</h4>
</div>
</div>
<div id="outline-container-org6613a75" class="outline-3">
<h3 id="org6613a75"><span class="section-number-3">3.2</span> Facebook Insights</h3>
<div class="outline-text-3" id="text-3-2">
<p>
统计分析。
</p>
</div>

<div id="outline-container-org88597e9" class="outline-4">
<h4 id="org88597e9"><span class="section-number-4">3.2.1</span> Realtime Analytics</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
The insights teams wanted to make statistics available to their users within seconds of user  actions rather than the hours previously supported. This  would require a large-scale, asynchronous queuing system for user actions as well as systems to process, aggregate, and persist these events. All of these systems need to be fault-tolerant and support more than a million events per second.
</p>
</div>
</div>

<div id="outline-container-orgc290388" class="outline-4">
<h4 id="orgc290388"><span class="section-number-4">3.2.2</span> High Throughput Increments</h4>
</div>
</div>
<div id="outline-container-orgf594f66" class="outline-3">
<h3 id="orgf594f66"><span class="section-number-3">3.3</span> Facebook Metrics System (ODS)</h3>
<div class="outline-text-3" id="text-3-3">
<p>
指标收集监控系统
</p>
</div>

<div id="outline-container-orgbd83a5f" class="outline-4">
<h4 id="orgbd83a5f"><span class="section-number-4">3.3.1</span> Automatic Sharding</h4>
</div>
<div id="outline-container-orga999883" class="outline-4">
<h4 id="orga999883"><span class="section-number-4">3.3.2</span> Fast Reads of Recent Data and Table Scans</h4>
</div>
</div>
</div>
<div id="outline-container-org07bc912" class="outline-2">
<h2 id="org07bc912"><span class="section-number-2">4</span> WHY HADOOP AND HBASE</h2>
<div class="outline-text-2" id="text-4">
<p>
The requirements for the storage  system  from  the  workloads presented above can be summarized  as follows (in no particular order):
</p>
<ol class="org-ol">
<li>Elasticity: We need to be able to add incremental capacity to our storage systems with minimal overhead and no downtime. In some cases we may want to add capacity rapidly and the system should automatically balance load and utilization across new hardware.  （扩展性好）</li>
<li>High write throughput: Most of the applications store (and optionally index) tremendous amounts of data and require high aggregate write throughput.（高吞吐写入）</li>
<li>Efficient and low-latency strong consistency semantics within a data center. While a globally distributed strongly consistent system is practically impossible, a system that could at least provide strong consistency within a data center would make it possible to provide a good user experience.（单机房里高效并且低延迟地达到强一致性）</li>
<li>Efficient random reads from disk:  In spite of the widespread use of application level caches (whether embedded or via memcached), at   Facebook scale, a lot of accesses miss the cache and hit the back-end storage system. MySQL is very efficient at performing random reads from disk and any new system would have to be comparable.（磁盘随机读需要高效）</li>
<li>High Availability and Disaster Recovery:  We need to provide a service with very high uptime to users that covers both planned and unplanned events  (examples of the former being events like software upgrades and addition of hardware/capacity and the latter exemplified by failures of hardware components). We also need to be able to tolerate the loss of a data center with minimal data loss and be able to serve data out of another data center in a reasonable time frame.（高可用性以及容灾）</li>
<li>Fault Isolation: Our long experience running large farms of MySQL databases has shown us that fault isolation is critical. Individual databases can and do  go  down,  but  only  a  small fraction of users are affected by any such event. Similarly, in our warehouse usage of Hadoop, individual disk failures affect only a small part of the data and the system quickly recovers from such faults.（错误隔离。即使出现问题的话只是部分数据受到影响，而其他数据依然可以正常访问和读写）</li>
<li>Atomic read-modify-write primitives:  Atomic increments and compare-and-swap APIs have been very useful in building lockless concurrent applications  and are a must have from the underlying storage system（这个可能对于计数是有用的）</li>
<li>Range Scans: Several applications require efficient retrieval of a set of rows in a particular range. For example all the last 100 messages for a given user or the hourly impression counts over the last 24 hours for a given advertiser.（范围扫描比如需要知道最后100条消息的时候可能有用）</li>
</ol>

<p>
It is also worth pointing out non-requirements:
</p>
<ol class="org-ol">
<li>Tolerance of network partitions within a single data center:  Different system components are often inherently centralized. For example, MySQL servers may all be located within a few racks, and network  partitions within a data center would cause major loss in serving capabilities therein. Hence every effort is made to eliminate the possibility of such events at the hardware level by having a highly redundant network design.（网络分割性的话通过在硬件层面解决比如使用高冗余的网络设计）</li>
<li>Zero Downtime in case of individual data center failure: In our experience such failures are very rare, though not impossible. In a less than ideal world where the choice of system design boils down to the choice of compromises that are acceptable, this is one compromise that we are willing to make given the low occurrence rate of such events</li>
<li>Active-active serving capability across different data centers: As mentioned before, we were comfortable making the assumption that user data could be federated across different data centers (based ideally on user locality). Latency (when user and data locality did not match up) could be masked by using an application cache close to the user.</li>
</ol>
</div>
</div>

<div id="outline-container-org64e000d" class="outline-2">
<h2 id="org64e000d"><span class="section-number-2">5</span> REALTIME HDFS</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org91fe6c5" class="outline-3">
<h3 id="org91fe6c5"><span class="section-number-3">5.1</span> High Availability - AvatarNode</h3>
<div class="outline-text-3" id="text-5-1">
<p>
NN hot standby.
</p>
<ul class="org-ul">
<li>fsimage的editlog通过NFS传递</li>
<li>多个avatar node之间通过zookeeper选举primary node,</li>
<li>dn会向多个avatar node做block report.</li>
</ul>
</div>
</div>

<div id="outline-container-orged531f7" class="outline-3">
<h3 id="orged531f7"><span class="section-number-3">5.2</span> Hadoop RPC compatibility</h3>
<div class="outline-text-3" id="text-5-2">
<p>
在RPC层面上兼容多个Hadoop版本存在
</p>
</div>
</div>

<div id="outline-container-org000d9e4" class="outline-3">
<h3 id="org000d9e4"><span class="section-number-3">5.3</span> Block Availability: Placement Policy</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>The default HDFS block placement policy, while rack aware, is still minimally constrained. Placement decision for non-local replicas is random, it can be on any rack and within any node of the rack.</li>
<li>To reduce the probability of data loss when multiple simultaneous nodes fail, we implemented a pluggable block placement policy that constrains the placement of block replicas into smaller, configurable node groups.（编写可插拔的策略. 将node进行分组来决定replicas放置位置） This allows us to reduce the probability of data loss by orders of magnitude, depending on the size chosen for the groups.</li>
<li>Our strategy is to define a window of racks and machines where replicas can be placed around the original block, using a logical ring of racks, each one containing a logical ring of machines. More details, the math, and the scripts used to calculate these numbers can be found at HDFS-1094. （这个策略我推断是这样的，首先将所有的rack做编号，然后每个rack内部的machine做编号。然后根据original block的位置，1th replica位置应该是在相同的rack但是不同的机器，这个机器和这个original block距离是某个windows size. 同理rack如此）</li>
<li>We found that the probability of losing a random block increases with the size of the node group. In our clusters, we started to use a node group of (2, 5), i.e. a rack window size of 2 and a machine window size of 5. We picked this choice because the probability of data loss is about a hundred times lesser than the default block placement policy.</li>
</ul>
</div>
</div>

<div id="outline-container-org1f53e6f" class="outline-3">
<h3 id="org1f53e6f"><span class="section-number-3">5.4</span> Performance Improvements for a Realtime Workload</h3>
<div class="outline-text-3" id="text-5-4">
<p>
HDFS is originally designed for high-throughput systems like MapReduce. <b>Many of its original design principles are to improve its throughput but do not focus much on response time.</b> For example, when dealing with errors, it favors retries or wait over fast failures. To support realtime applications, offering reasonable response time even in case of errors becomes the major challenge for HDFS.
</p>
</div>

<div id="outline-container-org24efa31" class="outline-4">
<h4 id="org24efa31"><span class="section-number-4">5.4.1</span> RPC Timeout</h4>
<div class="outline-text-4" id="text-5-4-1">
<ul class="org-ul">
<li>When a RPC client detects a tcp-socket timeout, instead of declaring a RPC timeout, it sends a ping to the RPC server. If the server is still alive, the client continues to wait for a response. （原有RPC实现是检测发生超时的话那么会发送一个ping检查RPC server是否存在，如果存在的话，那么依然会等待响应结果）
<ul class="org-ul">
<li>The idea is that if a RPC server is experiencing a communication burst, a temporary high load, or a stop the world GC, the client should wait and throttles its traffic to the server. （这是因为考虑到dn可能有高峰的负载或者是GC，所以client会等待并且自动调节和server的流量）</li>
<li>On the contrary, throwing a timeout exception or retrying the RPC request causes tasks to fail unnecessarily or add additional load to a RPC server.  （相反如果立即返回exception或者是重试的话，那么可能造成task不必要地失败，或者是对RPC server造成更大的负担）</li>
</ul></li>
<li>However, infinite wait adversely impacts any application that has a real time requirement. An HDFS client occasionally makes an RPC to some Dataode, and it is bad when the DataNode fails to respond back in time and the client is stuck in an RPC. （但是上面的策略会导致client stuck在某个RPC上，这对于实时系统是不可以接收的）
<ul class="org-ul">
<li>A better strategy is to fail fast and try a different DataNode for either reading or writing. （一个比较好的解决办法就是如果发现超时的话那么更换一台dn机器进行尝试）</li>
<li>Hence, we added the ability for specifying an RPC-timeout when starting a RPC session with a server.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgd9b4178" class="outline-4">
<h4 id="orgd9b4178"><span class="section-number-4">5.4.2</span> Recover File Lease</h4>
<div class="outline-text-4" id="text-5-4-2">
<ul class="org-ul">
<li>Another enhancement is to revoke a writer‘s lease quickly. 加快回收writer的租赁时间
<ul class="org-ul">
<li>HDFS supports only a single writer to a file and the NameNode maintains leases to enforce this semantic. （HDFS只允许每个文件一个writer）</li>
<li>There are many cases when an application wants to open a file to read but it was not closed cleanly earlier.（但是如果之前的writer没有正确清理而这个时候有reader的话，那么reader就需要等待这个writer尽快释放其lease)</li>
<li>Previously this was done by repetitively calling HDFS-append on the log file until the call succeeds. The append operations triggers a file’s soft lease to expire. So the application had to wait for a minimum of the soft lease period (with a default value of one minute) before the HDFS name node revokes the log file‘s lease.（开始是不断调用append来出发soft lease失效，但是失效也需要等待一段时间）</li>
<li>Secondly, the HDFS-append operation has additional unneeded  cost as establishing a write pipeline usually involves more than one DataNode. When an error occurs, a pipeline establishment might take up to 10 minutes. （而append本身也是存在不必要的开销需要建立write pipeline）</li>
</ul></li>
<li>To avoid the HDFS-append  overhead,  we  added  a  lightweight HDFS API called  recoverLease  that  revokes  a  file’s  lease explicitly. （增加API以及对应的语义来加快lease的失效和回收）
<ul class="org-ul">
<li>When the NameNode receives a recoverLease request, it immediately changes the fileBs lease holder to be itself. It then starts the lease recovery process.</li>
<li>The recoverLease rpc returns the status whether the lease recovery was complete. The application waits for a success return code from  recoverLease  before attempting to read from the file.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgc6e30dc" class="outline-4">
<h4 id="orgc6e30dc"><span class="section-number-4">5.4.3</span> Reads from Local Replicas</h4>
<div class="outline-text-4" id="text-5-4-3">
<ul class="org-ul">
<li>There are times when an application wants to store data in HDFS for scalability and performance reasons. However, the latency of reads and writes to an HDFS file is an order of magnitude greater than reading or writing to a local file on the machine.</li>
<li>To alleviate this problem, we implemented  an enhancement to the HDFS client that detects that there is a local replica of the data and then transparently reads data from the local replica without transferring the data via the DataNode. This has resulted in  doubling the performance profile of a certain workload that uses HBase.（如果发现本地有对应hdfs block的话那么直接从本地进行读取，而在走dn这层）</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org1d83926" class="outline-3">
<h3 id="org1d83926"><span class="section-number-3">5.5</span> New Features</h3>
<div class="outline-text-3" id="text-5-5">
</div>
<div id="outline-container-org0b3147e" class="outline-4">
<h4 id="org0b3147e"><span class="section-number-4">5.5.1</span> HDFS sync</h4>
<div class="outline-text-4" id="text-5-5-1">
<ul class="org-ul">
<li>Hflush/sync is an important operation for both HBase and Scribe. It pushes the written data buffered at the client side to the write pipeline, making the data visible to any new reader and increasing the data durability when either the client or any DataNode on the pipeline fails.（调用hflush/sync之后，之前所写的内容应该是全部都到了dn disk上面，能够被所有的reader读取到） #todo: 对hflush/sync这个语义至今比较模糊，一个比较主要的原因就是这个API历史上是调整过语义的</li>
<li>Hflush/sync is  a synchronous operation, meaning that it does not return until an acknowledgement from the write pipeline is received. Since the operation is frequently invoked, increasing its efficiency is important.（但是这个过程是同步的）</li>
<li>One optimization we have is to allow following writes to proceed while an Hflush/sync operation is waiting for a reply. This greatly increases the write throughput in both HBase and Scribe where a designated thread invokes Hflush/sync periodically.（一个优化就是在write的同时调用hflush/sync并且等待返回，这样可以增加写入的吞吐）</li>
</ul>
</div>
</div>

<div id="outline-container-org805cf07" class="outline-4">
<h4 id="org805cf07"><span class="section-number-4">5.5.2</span> Concurrent Readers</h4>
<div class="outline-text-4" id="text-5-5-2">
<ul class="org-ul">
<li>We have an application that requires the ability to read a file while it is being written to. The reader first talks to the NameNode to get the meta information of the file. Since the NameNode does not have the most updated information of its last block‘s length, the client fetches the information from one of the DataNodes where one of its replicas resides. It then starts to read the file.</li>
<li>The challenge of concurrent readers and writer is how to provision the last chunk of data when its data content and checksum are dynamically changing. We solve the problem by recomputing the checksum of the last chunk of data on demand.</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org01e7a69" class="outline-2">
<h2 id="org01e7a69"><span class="section-number-2">6</span> PRODUCTION HBASE</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgb339aca" class="outline-3">
<h3 id="orgb339aca"><span class="section-number-3">6.1</span> ACID Compliance</h3>
</div>
<div id="outline-container-orgcf51594" class="outline-3">
<h3 id="orgcf51594"><span class="section-number-3">6.2</span> Availability Improvements</h3>
<div class="outline-text-3" id="text-6-2">
</div>
<div id="outline-container-org357088b" class="outline-4">
<h4 id="org357088b"><span class="section-number-4">6.2.1</span> HBase Master Rewrite</h4>
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>We originally uncovered numerous issues during kill testing where HBase regions would go offline. We soon identified the problem: the transient state of the cluster is stored in the memory of the currently active HBase master only. Upon losing the master, this state is lost.</li>
<li>We undertook a large HBase master rewrite effort. The critical component of this rewrite was moving region assignment information from the master's in-memory state to ZooKeeper. Since ZooKeeper is quorum written to a majority of nodes, this transient state is not lost on master failover and can survive multiple server outages.（将一些中间状态比如region分配信息等写入到zookeeper里面，这样如果master失败的话那么重启的时候还能够恢复）</li>
</ul>
</div>
</div>

<div id="outline-container-orge97d018" class="outline-4">
<h4 id="orge97d018"><span class="section-number-4">6.2.2</span> Online Upgrades</h4>
<div class="outline-text-4" id="text-6-2-2">
<ul class="org-ul">
<li>The largest cause of cluster  downtime was not random server deaths, but rather system maintenance. We had a number of problems to solve to minimize this downtime.  （系统维护是整个服务down掉的最主要的因素）</li>
<li>First, we discovered over time that RegionServers would intermittently require minutes to shutdown after issuing a stop request. This intermittent problem was caused by long compaction cycles. To address this, we made compactions interruptible to favor responsiveness over completion. This reduced RegionServer downtime  to seconds and gave us a reasonable bound on cluster shutdown time. （通常在stop之前最要做一个compaction, 所以在发起stop到真正完全down掉期间会有很长的停顿。我们修改compaction使得整个过程可以中断，这样stop的时间就变得可空）</li>
<li>Another availability improvement was rolling restarts. Originally, HBase only supported full cluster stop and start for upgrades. We added rolling restarts script to  perform  software  upgrades  one server at a time. Since the master automatically reassigns regions on a RegionServer stop, this minimizes the amount of downtime that our users experience. （修改启动脚本能够让整个集群滚动地启动，而因为hbase每次rs下面都会做rebalance, 因此整个过程是非常快速的）
<ul class="org-ul">
<li>We fixed numerous edge case issues that resulted from this new restart. Incidentally, numerous bugs during rolling restarts were related to region offlining and reassignment, so our master rewrite with ZooKeeper integration helped address a number of issues here as well. （在offlining和reassignment的过程中出现了很多边界情况，通过将master状态写到zookeeper这个实现有助于定位和解决问题）</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org08ef837" class="outline-4">
<h4 id="org08ef837"><span class="section-number-4">6.2.3</span> Distributed Log Splitting</h4>
</div>
</div>
<div id="outline-container-org9e53f37" class="outline-3">
<h3 id="org9e53f37"><span class="section-number-3">6.3</span> Performance Improvements</h3>
<div class="outline-text-3" id="text-6-3">
</div>
<div id="outline-container-org6659658" class="outline-4">
<h4 id="org6659658"><span class="section-number-4">6.3.1</span> Compaction</h4>
<div class="outline-text-4" id="text-6-3-1">
<ul class="org-ul">
<li>The next task was improving the compaction algorithm. We  discovered  a  pathological  case where a 1 GB file would be regularly compacted with three 5 MB files to produce a slightly larger file. This network IO waste would continue until the compaction queue started to backlog. This problem occurred because the existing algorithm would unconditionally minor compact the first four HFiles, while triggering a minor compaction after 3 HFiles had been reached. The solution was to stop unconditionally compacting files above a certain size and skip compactions if enough candidate files could not be found. Afterwards,  our put latency dropped from 25 milliseconds to 3 milliseconds.</li>
<li>We also worked on improving the size ratio decision of the compaction algorithm. Originally, the compaction algorithm would sort by file age and compare adjacent files. If the older file was less than 2x the size of the newer file, the compaction algorithm with include this file and iterate. However, this algorithm had suboptimal behavior as the number and size of HFiles increased significantly. To improve, we now include an older file if it is within 2x the aggregate size of all newer HFiles. This transforms the steady state  so that an old HFile will be roughly 4x the size of the next newer file, and we consequently have a steeper curve while still maintaining a 50% compaction ratio.</li>
</ul>
</div>
</div>

<div id="outline-container-orgedc5a9e" class="outline-4">
<h4 id="orgedc5a9e"><span class="section-number-4">6.3.2</span> Read Optimizations</h4>
<div class="outline-text-4" id="text-6-3-2">
<ul class="org-ul">
<li>As discussed, read performance hinges on keeping the number of files in a region low thus reducing random IO operations. In addition to utilizing comapctions to keep the number of files on disk low, it is also possible to skip certain files for some queries, similarly reducing IO operations.（减少sstable文件）</li>
<li>Bloom filters provide a space-efficient and constant-time method for checking if a given row or row and column exists in a given HFile.</li>
<li>For data stored in HBase that is time-series or contains a specific, known timestamp, a special timestamp file selection algorithm was added. Since time moves forward and data is rarely inserted at a significantly later time than its timestamp, each HFile will generally contain values for a fixed range of time. This information is stored as metadata in each HFile and queries that ask for a specific timestamp or range of timestamps will check if the request intersects with the ranges of each file, skipping those which do not overlap.（直接在HFile里面添加timestamp信息来做过滤）</li>
<li>As read performance improved significantly with HDFS local file reads, it is critical that regions are hosted on the same physical nodes as their files. Changes have been made to retain the assignment of regions across cluster and node restarts to ensure that locality is maintained</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org35c7520" class="outline-2">
<h2 id="org35c7520"><span class="section-number-2">7</span> DEPLOYMENT AND OPERATIONAL EXPERIENCES</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orge7570eb" class="outline-3">
<h3 id="orge7570eb"><span class="section-number-3">7.1</span> Testing</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>From early on in our design of an HBase solution, we were worried about code stability. We first needed to test the stability and durability of the open source HBase code and additionally ensure the stability of our future changes. To this end, we wrote an HBase testing program. The testing program generated data to write into HBase, both deterministically and randomly. The tester will write data into the HBase cluster and simultaneously read and verify all the data it has added. （对于HBase读写做正确性验证）We further enhanced the tester to randomly select and kill processes in  the  cluster  and  verify  that successfully returned database transactions were indeed written. This helped catch a lot of issues, and is still our first method of testing changes（并且随机杀掉一些进程来验证数据是否正确）</li>
<li>Although our common cluster contains many servers operating in a distributed fashion, our local development verification commonly consists of unit tests and single-server setups. We were concerned about discrepancies between single-server setups and truly distributed scenarios. We created a utility called HBase Verify to run simple CRUD workloads on a live server. This allows us to exercise simple API calls and run load tests in a couple of minutes. This utility is  even  more  important  for  our dark launch clusters, where algorithms are first evaluated at a large scale.（对于单机使用了unittest以及a live-server上面验证CRUD操作是否正确）</li>
</ul>
</div>
</div>

<div id="outline-container-orgf7b5f91" class="outline-3">
<h3 id="orgf7b5f91"><span class="section-number-3">7.2</span> Monitoring and Tools</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>As we gained more experience with production usage of HBase, it became clear that our primary problem was in consistent assignment of regions to RegionServers. Two RegionServers could end up serving the same region, or a region may be left unassigned.（region assignement不一致，会造成某个region被两个rs共同管理，或者是某个region没有rs来管理）
<ul class="org-ul">
<li>These problems are characterized by inconsistencies in metadata about the state of the regions that are stored in different places: the META region in HBase, ZooKeeper, files corresponding to a region in HDFS and the in-memory state of the RegionServers.</li>
<li>To that end, we created HBCK as a database-level FSCK utility to verify the consistency between these different sources  of  metadata.  For  the  common inconsistencies, we added an HBCK ifixB option to clear the inmemory state and have the HMaster reassign the inconsistent region. Nowadays we run HBCK almost continuously against our production clusters to catch problems as early as possible.</li>
</ul></li>
<li>A critical component for cluster monitoring is operational metrics. In particular, RegionServer metrics are far more useful for evaluating the health of the cluster than HMaster or ZooKeeper metrcs. HBase already had a number of metrics exported through JMX. However, all the metrics were for short-running operations such as log writes and RPC requests. We needed to add metrics to monitor long-running events such as compactions, flushes, and log splits. A slightly innocuous metric that ended up being critical for monitoring was version information. We have multiple clusters that often have divergent versions.（版本是比较重要的监控信息，因为每个版本功能是不同的，而整个集群里面可能会存在很多不同的版本）</li>
</ul>
</div>
</div>

<div id="outline-container-orgd7df8e3" class="outline-3">
<h3 id="orgd7df8e3"><span class="section-number-3">7.3</span> Manual versus Automatic Splitting</h3>
<div class="outline-text-3" id="text-7-3">
<p>
#note: manual splitting over automatic splitting是有一定前提假设的，不过确实带来了许多可控方面的好处
</p>

<ul class="org-ul">
<li>Since our data grows roughly uniform across all regions, it's easy for automatic splitting to cause split and compaction storms as the regions all roughly hit the same data size at the same time. With manual splits, we can stagger splits across time and thereby spread out the network IO load typically generated by the splitting process. This minimizes impact to production workload.</li>
<li>Since the number of regions is known at any given point in time, long-term debugging and profiling is much easier. It is hard to trace the logs to understand region level problems if regions keep splitting and getting renamed.</li>
</ul>
</div>
</div>

<div id="outline-container-orgcfbd9a5" class="outline-3">
<h3 id="orgcfbd9a5"><span class="section-number-3">7.4</span> Dark Launch</h3>
<div class="outline-text-3" id="text-7-4">
<ul class="org-ul">
<li>Migrating from a legacy messaging system offered one major advantage: real-world testing capability. At Facebook, we widely use a testing/rollout process called "Dark Launch" where critical back-end functionality is exercised by a subset of the user base without exposing any UI changes to them. We used this facility to double-write messaging traffic for some users to both the legacy infrastructure and HBase.（将一部分用户请求重写一份到新的集群上面）
<ul class="org-ul">
<li>This allowed us to do useful performance benchmarks and find  practical  HBase  bottlenecks instead of relying purely on artificial benchmarks and estimations. （使用真实流量测试性能）</li>
<li>Even after product launch, we still found many uses for Dark Launch clusters. All code changes normally spend a week running on Dark Launch before a production push is considered. （正式发布是在暗启动正常工作1周后上线）</li>
<li>Additionally, Dark Launch normally handles at least 2x the load that we expect our production clusters to handle. Long term testing at 2x load allows us to weather multiple traffic spikes and verify that HBase can handle outlier peak conditions before we vertically scale.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgaa44bd6" class="outline-3">
<h3 id="orgaa44bd6"><span class="section-number-3">7.5</span> Dashboards/ODS integration</h3>
</div>
<div id="outline-container-orgae4dc9d" class="outline-3">
<h3 id="orgae4dc9d"><span class="section-number-3">7.6</span> Backups at the Application layer</h3>
</div>
<div id="outline-container-org94351b1" class="outline-3">
<h3 id="org94351b1"><span class="section-number-3">7.7</span> Schema Changes</h3>
</div>
<div id="outline-container-org77ab2df" class="outline-3">
<h3 id="org77ab2df"><span class="section-number-3">7.8</span> Importing Data</h3>
</div>
<div id="outline-container-org224dad2" class="outline-3">
<h3 id="org224dad2"><span class="section-number-3">7.9</span> Reducing Network IO</h3>
<div class="outline-text-3" id="text-7-9">
<ul class="org-ul">
<li>After running in production for a couple months, we quickly realized from our dashboards that we were network IO bound. We needed some way to analyze where our network IO traffic was coming from.</li>
<li>We utilized a combination of JMX statistics and log scraping to estimate total network IO on a single RegionServer for a 24-hour period. We broke down the network traffic across the MemStore flush (15%), size-based minor compactions (38%), and time-based major compactions (47%).（通过JMX来观察网路流量的开销来进行优化） We found a lot of lowhanging optimizations by observing these ratios.
<ul class="org-ul">
<li>We were able to get 40% network IO reduction by simply increasing our major compaction interval from every day to every week.</li>
<li>We also got big gains by excluding certain column families from being logged to the HLog. Best effort durability sufficed for data stored in these column families</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org42c0b49" class="outline-2">
<h2 id="org42c0b49"><span class="section-number-2">8</span> FUTURE WORK</h2>
<div class="outline-text-2" id="text-8">
<ul class="org-ul">
<li>adding support for maintenance of secondary indices and summary views in HBase. In many use cases, such derived data and views can be maintained asynchronously （维护二级索引以及摘要等信息，这些信息通常都是可以异步维护的）</li>
<li>Many use cases benefit from storing a large amount of data in HBaseBs cache and improvements to HBase are required to exploit very large physical memory. The current limitations in this area arise from issues with using an extremely large heap in Java and we are evaluating several proposals like writing a slab allocator in Java or managing memory via JNI（使用JNI来改写slab allocator)</li>
<li>A related topic  is exploiting flash memory to extend the HBase cache and we are exploring various ways to utilize it including <a href="https://github.com/facebook/flashcache%20">FlashCache</a></li>
<li>Finally, as we try to use Hadoop and HBase for applications that are built to serve the same data in an active-active manner across different data centers, we are exploring approaches to deal with multi data-center replication and conflict resolution（多机房部署使用以及冲突处理）</li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: work</p>
<p class="date">Created: 2019-04-03 Wed 04:32</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
<div id="content"><!-- DISQUS BEGIN --><div id="disqus_thread"></div><script>/***  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/var disqus_config = function () {this.page.url = 'https://maypeppa.github.io/html/apache-hadoop-goes-realtime-at-facebook.html';this.page.identifier = 'apache-hadoop-goes-realtime-at-facebook.html';};(function() {var d = document, s = d.createElement('script');s.src = 'https://dirlt.disqus.com/embed.js';s.setAttribute('data-timestamp', +new Date());(d.head || d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><!-- DISQUS END --></div></body>
</html>

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 43043: expected 2 fields, saw 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('labeledTrainData.tsv', delimiter='\\t')\n",
    "df2 = pd.read_csv('unlabeledTrainData.tsv', delimiter='\\t', error_bad_lines= False)\n",
    "df_test = pd.read_csv('testData.tsv', delimiter= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "def review_to_words(txt):\n",
    "    # comment following to speed up.\n",
    "    # bs = BeautifulSoup(txt)\n",
    "    # txt = re.sub('[^a-zA-Z]', ' ', bs.get_text())\n",
    "    words = filter(lambda x: x and x.isalpha() and len(x) >= 3 and x not in english_stopwords, txt.lower().split())\n",
    "    # words = filter(lambda x: x and x not in english_stopwords, txt.lower().split())\n",
    "    # return ' '.join(words) # for BOW\n",
    "    return words # for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['clean_review'] = df['review'].apply(review_to_words)\n",
    "df2['clean_review'] = df2['review'].apply(review_to_words)\n",
    "df_test['clean_review'] = df_test['review'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_reviewes = pd.Series.append(df['clean_review'], df2['clean_review'])\n",
    "clean_reviewes = clean_reviewes.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(min_df = 3, max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv.fit(clean_reviewes)\n",
    "tfidf = tv.transform(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16750, 500), (16750,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = tfidf, df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6cf6d09157bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    585\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    586\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr =  0.853669783999\n",
      "svc =  0.862282871846\n"
     ]
    }
   ],
   "source": [
    "# y_pred = nb.predict(X_test)\n",
    "# print 'nb = ', roc_auc_score(y_pred, y_test)\n",
    "y_pred = lr.predict(X_test)\n",
    "print 'lr = ', roc_auc_score(y_pred, y_test)\n",
    "y_pred = svc.predict(X_test)\n",
    "print 'svc = ', roc_auc_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb.fit(X, y)\n",
    "lr.fit(X, y)\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tfidf_test = tv.transform(df_test['clean_review'])\n",
    "tfidf_test = compute_tfidf(df_test['clean_review'])\n",
    "y_output = svc.predict(tfidf_test)\n",
    "df_output = pd.DataFrame({'id': df_test['id'], 'sentiment': y_output}, columns=('id', 'sentiment'))\n",
    "df_output.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__try word2vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-25 19:08:47,451 : INFO : collecting all words and their counts\n",
      "2016-10-25 19:08:47,452 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-25 19:08:47,704 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2016-10-25 19:08:47,946 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2016-10-25 19:08:48,190 : INFO : PROGRESS: at sentence #30000, processed 3584935 words, keeping 81517 word types\n",
      "2016-10-25 19:08:48,463 : INFO : PROGRESS: at sentence #40000, processed 4782315 words, keeping 93232 word types\n",
      "2016-10-25 19:08:48,808 : INFO : PROGRESS: at sentence #50000, processed 5994110 words, keeping 103318 word types\n",
      "2016-10-25 19:08:49,171 : INFO : PROGRESS: at sentence #60000, processed 7199355 words, keeping 112019 word types\n",
      "2016-10-25 19:08:49,449 : INFO : PROGRESS: at sentence #70000, processed 8392228 words, keeping 119672 word types\n",
      "2016-10-25 19:08:49,584 : INFO : collected 123345 word types from a corpus of 8992035 raw words and 74998 sentences\n",
      "2016-10-25 19:08:49,584 : INFO : Loading a fresh vocabulary\n",
      "2016-10-25 19:08:49,686 : INFO : min_count=40 retains 16338 unique words (13% of original 123345, drops 107007)\n",
      "2016-10-25 19:08:49,687 : INFO : min_count=40 leaves 8432963 word corpus (93% of original 8992035, drops 559072)\n",
      "2016-10-25 19:08:49,749 : INFO : deleting the raw counts dictionary of 123345 items\n",
      "2016-10-25 19:08:49,754 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2016-10-25 19:08:49,756 : INFO : downsampling leaves estimated 8063206 word corpus (95.6% of prior 8432963)\n",
      "2016-10-25 19:08:49,757 : INFO : estimated required memory for 16338 words and 300 dimensions: 47380200 bytes\n",
      "2016-10-25 19:08:49,818 : INFO : resetting layer weights\n",
      "2016-10-25 19:08:50,107 : INFO : training model with 4 workers on 16338 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2016-10-25 19:08:50,107 : INFO : expecting 74998 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-10-25 19:08:51,126 : INFO : PROGRESS: at 1.69% examples, 682040 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:08:52,145 : INFO : PROGRESS: at 3.31% examples, 667207 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:08:53,135 : INFO : PROGRESS: at 4.89% examples, 654188 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:08:54,148 : INFO : PROGRESS: at 6.77% examples, 674854 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:08:55,152 : INFO : PROGRESS: at 8.59% examples, 685064 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:08:56,168 : INFO : PROGRESS: at 10.44% examples, 693300 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:08:57,179 : INFO : PROGRESS: at 12.15% examples, 693252 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:08:58,193 : INFO : PROGRESS: at 13.73% examples, 685190 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:08:59,216 : INFO : PROGRESS: at 15.29% examples, 678507 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:00,229 : INFO : PROGRESS: at 16.89% examples, 673890 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:01,246 : INFO : PROGRESS: at 18.59% examples, 672973 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:02,253 : INFO : PROGRESS: at 20.41% examples, 677949 words/s, in_qsize 8, out_qsize 1\n",
      "2016-10-25 19:09:03,261 : INFO : PROGRESS: at 22.24% examples, 682635 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:04,273 : INFO : PROGRESS: at 24.07% examples, 685985 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:05,273 : INFO : PROGRESS: at 25.93% examples, 689871 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:06,279 : INFO : PROGRESS: at 28.03% examples, 698538 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:07,300 : INFO : PROGRESS: at 29.89% examples, 700649 words/s, in_qsize 7, out_qsize 2\n",
      "2016-10-25 19:09:08,299 : INFO : PROGRESS: at 31.75% examples, 703465 words/s, in_qsize 8, out_qsize 1\n",
      "2016-10-25 19:09:09,307 : INFO : PROGRESS: at 33.49% examples, 703456 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:10,308 : INFO : PROGRESS: at 35.01% examples, 699392 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:11,313 : INFO : PROGRESS: at 36.60% examples, 696423 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:12,326 : INFO : PROGRESS: at 38.23% examples, 693866 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:13,328 : INFO : PROGRESS: at 40.06% examples, 695744 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:14,346 : INFO : PROGRESS: at 41.91% examples, 697615 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:15,366 : INFO : PROGRESS: at 43.77% examples, 699510 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:16,380 : INFO : PROGRESS: at 45.66% examples, 701052 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:17,387 : INFO : PROGRESS: at 47.54% examples, 702504 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:18,395 : INFO : PROGRESS: at 49.40% examples, 703833 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:19,418 : INFO : PROGRESS: at 51.27% examples, 705033 words/s, in_qsize 8, out_qsize 1\n",
      "2016-10-25 19:09:20,421 : INFO : PROGRESS: at 53.07% examples, 705974 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:21,434 : INFO : PROGRESS: at 54.93% examples, 707243 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:22,450 : INFO : PROGRESS: at 56.81% examples, 708381 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:23,460 : INFO : PROGRESS: at 58.67% examples, 709316 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:24,472 : INFO : PROGRESS: at 60.54% examples, 710429 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:25,487 : INFO : PROGRESS: at 62.39% examples, 711337 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:26,498 : INFO : PROGRESS: at 64.23% examples, 711883 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:27,508 : INFO : PROGRESS: at 66.11% examples, 712807 words/s, in_qsize 8, out_qsize 1\n",
      "2016-10-25 19:09:28,511 : INFO : PROGRESS: at 67.98% examples, 713595 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:29,515 : INFO : PROGRESS: at 69.83% examples, 714123 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:30,526 : INFO : PROGRESS: at 71.63% examples, 714484 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:31,544 : INFO : PROGRESS: at 73.49% examples, 715112 words/s, in_qsize 8, out_qsize 1\n",
      "2016-10-25 19:09:32,547 : INFO : PROGRESS: at 75.12% examples, 713912 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:33,563 : INFO : PROGRESS: at 77.00% examples, 714619 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:34,577 : INFO : PROGRESS: at 78.63% examples, 712907 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:35,577 : INFO : PROGRESS: at 80.21% examples, 711316 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:36,589 : INFO : PROGRESS: at 82.06% examples, 712057 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:37,605 : INFO : PROGRESS: at 83.93% examples, 712728 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:38,609 : INFO : PROGRESS: at 85.79% examples, 713344 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:39,622 : INFO : PROGRESS: at 87.70% examples, 713977 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:40,627 : INFO : PROGRESS: at 89.54% examples, 714357 words/s, in_qsize 7, out_qsize 1\n",
      "2016-10-25 19:09:41,653 : INFO : PROGRESS: at 91.34% examples, 714472 words/s, in_qsize 8, out_qsize 1\n",
      "2016-10-25 19:09:42,643 : INFO : PROGRESS: at 92.89% examples, 712955 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:43,648 : INFO : PROGRESS: at 94.82% examples, 714168 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:09:44,656 : INFO : PROGRESS: at 96.64% examples, 714494 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:09:45,666 : INFO : PROGRESS: at 98.38% examples, 714118 words/s, in_qsize 7, out_qsize 1\n",
      "2016-10-25 19:09:46,521 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-10-25 19:09:46,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-10-25 19:09:46,549 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-10-25 19:09:46,553 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-10-25 19:09:46,555 : INFO : training on 44960175 raw words (40315209 effective words) took 56.4s, 714329 effective words/s\n",
      "2016-10-25 19:09:46,556 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-10-25 19:09:46,709 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2016-10-25 19:09:46,710 : INFO : not storing attribute syn0norm\n",
      "2016-10-25 19:09:46,711 : INFO : not storing attribute cum_table\n",
      "2016-10-25 19:09:46,863 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(clean_reviewes, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def words_to_vec(model, words):\n",
    "    vec = np.zeros(model.vector_size)\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        # print w\n",
    "        if w not in model.vocab: continue\n",
    "        v = model[w]\n",
    "        vec += v\n",
    "        count += 1\n",
    "    if count:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_tfidf(review):\n",
    "    tfidf = review.apply(lambda x: words_to_vec(model, x))\n",
    "    tmp = []\n",
    "    for x in tfidf:\n",
    "        tmp.append(x.tolist())\n",
    "    tfidf = np.array(tmp)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = compute_tfidf(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_test = compute_tfidf(df_test['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# whish NB can deals with data.\n",
    "tfidf = scaler.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__try doc2vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-25 19:29:35,108 : INFO : collecting all words and their counts\n",
      "2016-10-25 19:29:35,109 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2016-10-25 19:29:35,246 : INFO : PROGRESS: at example #10000, processed 905662 words (6614201/s), 42242 word types, 10000 tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-25 19:29:35,402 : INFO : PROGRESS: at example #20000, processed 1801245 words (5764451/s), 55223 word types, 20000 tags\n",
      "2016-10-25 19:29:35,560 : INFO : PROGRESS: at example #30000, processed 2695466 words (5707121/s), 65815 word types, 30000 tags\n",
      "2016-10-25 19:29:35,722 : INFO : PROGRESS: at example #40000, processed 3596299 words (5575872/s), 74658 word types, 40000 tags\n",
      "2016-10-25 19:29:35,886 : INFO : PROGRESS: at example #50000, processed 4506734 words (5573560/s), 82079 word types, 50000 tags\n",
      "2016-10-25 19:29:36,047 : INFO : PROGRESS: at example #60000, processed 5411873 words (5638689/s), 88445 word types, 60000 tags\n",
      "2016-10-25 19:29:36,207 : INFO : PROGRESS: at example #70000, processed 6309853 words (5622845/s), 94195 word types, 70000 tags\n",
      "2016-10-25 19:29:36,300 : INFO : collected 96834 word types and 74998 unique tags from a corpus of 74998 examples and 6760780 words\n",
      "2016-10-25 19:29:36,300 : INFO : Loading a fresh vocabulary\n",
      "2016-10-25 19:29:36,399 : INFO : min_count=40 retains 13114 unique words (13% of original 96834, drops 83720)\n",
      "2016-10-25 19:29:36,401 : INFO : min_count=40 leaves 6298708 word corpus (93% of original 6760780, drops 462072)\n",
      "2016-10-25 19:29:36,463 : INFO : deleting the raw counts dictionary of 96834 items\n",
      "2016-10-25 19:29:36,471 : INFO : sample=0.001 downsamples 29 most-common words\n",
      "2016-10-25 19:29:36,472 : INFO : downsampling leaves estimated 5969964 word corpus (94.8% of prior 6298708)\n",
      "2016-10-25 19:29:36,473 : INFO : estimated required memory for 13114 words and 500 dimensions: 211631800 bytes\n",
      "2016-10-25 19:29:36,497 : INFO : constructing a huffman tree from 13114 words\n",
      "2016-10-25 19:29:36,886 : INFO : built huffman tree with maximum node depth 17\n",
      "2016-10-25 19:29:36,892 : INFO : resetting layer weights\n",
      "2016-10-25 19:29:38,527 : INFO : training model with 4 workers on 13114 vocabulary and 500 features, using sg=0 hs=1 sample=0.001 negative=0 window=10\n",
      "2016-10-25 19:29:38,528 : INFO : expecting 74998 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-10-25 19:29:39,602 : INFO : PROGRESS: at 1.18% examples, 339116 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:40,629 : INFO : PROGRESS: at 2.47% examples, 358950 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:41,674 : INFO : PROGRESS: at 3.76% examples, 363761 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:42,711 : INFO : PROGRESS: at 4.95% examples, 358666 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:43,713 : INFO : PROGRESS: at 6.14% examples, 357869 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:29:44,748 : INFO : PROGRESS: at 7.39% examples, 358342 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:45,749 : INFO : PROGRESS: at 8.49% examples, 354200 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:46,826 : INFO : PROGRESS: at 9.64% examples, 349953 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:47,853 : INFO : PROGRESS: at 10.90% examples, 352394 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:48,863 : INFO : PROGRESS: at 12.11% examples, 354021 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:49,891 : INFO : PROGRESS: at 13.36% examples, 355567 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:29:50,901 : INFO : PROGRESS: at 14.48% examples, 353837 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:51,920 : INFO : PROGRESS: at 15.74% examples, 355458 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:29:52,930 : INFO : PROGRESS: at 17.02% examples, 357624 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:29:53,959 : INFO : PROGRESS: at 18.10% examples, 354545 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:29:54,974 : INFO : PROGRESS: at 19.13% examples, 351514 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:55,996 : INFO : PROGRESS: at 20.21% examples, 349760 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:57,008 : INFO : PROGRESS: at 21.24% examples, 347773 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:58,009 : INFO : PROGRESS: at 22.35% examples, 347163 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:29:59,042 : INFO : PROGRESS: at 23.36% examples, 344408 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:00,055 : INFO : PROGRESS: at 24.37% examples, 342215 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:01,092 : INFO : PROGRESS: at 25.43% examples, 340684 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:02,124 : INFO : PROGRESS: at 26.47% examples, 338962 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:03,130 : INFO : PROGRESS: at 27.84% examples, 341740 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:04,138 : INFO : PROGRESS: at 29.13% examples, 343526 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:05,140 : INFO : PROGRESS: at 30.34% examples, 344297 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:06,168 : INFO : PROGRESS: at 31.67% examples, 346263 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:07,174 : INFO : PROGRESS: at 32.89% examples, 347117 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:08,178 : INFO : PROGRESS: at 34.18% examples, 348544 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:09,195 : INFO : PROGRESS: at 35.40% examples, 349151 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:10,218 : INFO : PROGRESS: at 36.71% examples, 350232 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:11,224 : INFO : PROGRESS: at 37.94% examples, 350855 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:12,243 : INFO : PROGRESS: at 39.21% examples, 351562 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:13,252 : INFO : PROGRESS: at 40.64% examples, 353876 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:14,277 : INFO : PROGRESS: at 41.97% examples, 355078 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:15,279 : INFO : PROGRESS: at 43.27% examples, 356014 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:16,309 : INFO : PROGRESS: at 44.57% examples, 356653 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:17,337 : INFO : PROGRESS: at 45.92% examples, 357752 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:18,341 : INFO : PROGRESS: at 47.27% examples, 358779 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:19,373 : INFO : PROGRESS: at 48.66% examples, 359943 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:20,388 : INFO : PROGRESS: at 50.01% examples, 360975 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:21,394 : INFO : PROGRESS: at 51.25% examples, 361218 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:22,407 : INFO : PROGRESS: at 52.28% examples, 360134 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:23,413 : INFO : PROGRESS: at 53.48% examples, 360172 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:24,428 : INFO : PROGRESS: at 54.71% examples, 360343 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:25,433 : INFO : PROGRESS: at 56.09% examples, 361532 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:26,449 : INFO : PROGRESS: at 57.34% examples, 361656 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:27,468 : INFO : PROGRESS: at 58.71% examples, 362642 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:28,484 : INFO : PROGRESS: at 59.95% examples, 362732 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:29,500 : INFO : PROGRESS: at 61.21% examples, 363107 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:30,519 : INFO : PROGRESS: at 62.50% examples, 363489 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:31,520 : INFO : PROGRESS: at 63.70% examples, 363479 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:32,535 : INFO : PROGRESS: at 65.19% examples, 364895 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:33,553 : INFO : PROGRESS: at 66.38% examples, 364598 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:34,615 : INFO : PROGRESS: at 67.80% examples, 365297 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:35,629 : INFO : PROGRESS: at 69.05% examples, 365340 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:36,658 : INFO : PROGRESS: at 70.22% examples, 364997 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:37,676 : INFO : PROGRESS: at 71.62% examples, 365919 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:38,708 : INFO : PROGRESS: at 72.89% examples, 366122 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:39,730 : INFO : PROGRESS: at 74.16% examples, 366257 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:40,734 : INFO : PROGRESS: at 75.26% examples, 365771 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:41,741 : INFO : PROGRESS: at 76.50% examples, 365852 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:42,775 : INFO : PROGRESS: at 77.62% examples, 365212 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:43,799 : INFO : PROGRESS: at 78.94% examples, 365600 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:44,818 : INFO : PROGRESS: at 80.09% examples, 365208 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:45,823 : INFO : PROGRESS: at 81.33% examples, 365381 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:46,831 : INFO : PROGRESS: at 82.63% examples, 365698 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:47,868 : INFO : PROGRESS: at 84.06% examples, 366495 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:48,899 : INFO : PROGRESS: at 85.38% examples, 366694 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:49,915 : INFO : PROGRESS: at 86.63% examples, 366697 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:50,925 : INFO : PROGRESS: at 87.84% examples, 366625 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:51,931 : INFO : PROGRESS: at 89.07% examples, 366678 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:52,997 : INFO : PROGRESS: at 90.37% examples, 366681 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:54,022 : INFO : PROGRESS: at 91.64% examples, 366876 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:55,027 : INFO : PROGRESS: at 92.86% examples, 366920 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:56,077 : INFO : PROGRESS: at 94.16% examples, 366996 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:57,088 : INFO : PROGRESS: at 95.43% examples, 367251 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:30:58,102 : INFO : PROGRESS: at 96.74% examples, 367484 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:30:59,106 : INFO : PROGRESS: at 98.06% examples, 367867 words/s, in_qsize 7, out_qsize 0\n",
      "2016-10-25 19:31:00,118 : INFO : PROGRESS: at 99.33% examples, 367977 words/s, in_qsize 8, out_qsize 0\n",
      "2016-10-25 19:31:00,604 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-10-25 19:31:00,624 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-10-25 19:31:00,658 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-10-25 19:31:00,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-10-25 19:31:00,662 : INFO : training on 33803900 raw words (30224492 effective words) took 82.1s, 368010 effective words/s\n",
      "2016-10-25 19:31:00,664 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-10-25 19:31:00,796 : INFO : saving Doc2Vec object under 300features_40minwords_10context, separately None\n",
      "2016-10-25 19:31:00,796 : INFO : storing numpy array 'doctag_syn0' to 300features_40minwords_10context.docvecs.doctag_syn0.npy\n",
      "2016-10-25 19:31:01,036 : INFO : not storing attribute syn0norm\n",
      "2016-10-25 19:31:01,037 : INFO : not storing attribute cum_table\n",
      "2016-10-25 19:31:01,306 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 500    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "documents = map(lambda (idx, x): doc2vec.TaggedDocument(x, [idx]), enumerate(clean_reviewes))\n",
    "print \"Training model...\"\n",
    "model = doc2vec.Doc2Vec(documents, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
